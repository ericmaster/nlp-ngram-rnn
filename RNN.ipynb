{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# Language Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language models powered by n-grams and RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEQByNzNI4d",
        "outputId": "0a02ebec-4eab-4e46-c183-2849c72630bd"
      },
      "outputs": [],
      "source": [
        "# !unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "57f5f2c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fda0b2",
      "metadata": {
        "id": "06fda0b2"
      },
      "source": [
        "### Step 2: RNN Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454617c7",
      "metadata": {
        "id": "454617c7"
      },
      "source": [
        "#### Preparing the Data\n",
        "The following Python code is used for loading and processing [GloVe (Global Vectors for Word Representation) embeddings](https://nlp.stanford.edu/projects/glove/). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. These embeddings can be used in various natural language processing and machine learning tasks.\n",
        "\n",
        "The `load_glove_embeddings(path)` function is used to load the GloVe embeddings from a file. The function takes a file path as an argument, reads the file line by line, and for each line, it splits the line into words and their corresponding embeddings, and stores them in a dictionary. The dictionary, embeddings_dict, maps words to their corresponding vector representations.\n",
        "\n",
        "The `create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim)` function is used to create an embedding matrix from the loaded GloVe embeddings. This function takes a dictionary mapping words to their indices (`word_to_ix`), the dictionary of GloVe embeddings (`embeddings_dict`), and the dimension of the embeddings (`embedding_dim`) as arguments. It creates a zero matrix of size (vocab_size, embedding_dim) and then for each word in  `word_to_ix`, it checks if the word is in `embeddings_dict`. If it is, it assigns the corresponding GloVe vector to the word's index in the embedding matrix. If the word is not in the embeddings_dict, it assigns a random vector to the word's index in the embedding matrix.\n",
        "\n",
        "The `glove_path` variable is the path to the GloVe file, and `glove_embeddings` is the dictionary of GloVe embeddings loaded using the `load_glove_embeddings` function. The `embedding_dim` variable is the dimension of the embeddings, and `embedding_matrix` is the embedding matrix created using the create_embedding_matrix function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a4411999",
      "metadata": {
        "id": "a4411999"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "# vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/sample.txt\")\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/ed_sheeran.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aQ3a37oj1-F6",
      "metadata": {
        "id": "aQ3a37oj1-F6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9e9b6d54",
      "metadata": {
        "id": "9e9b6d54"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Path to the GloVe file\n",
        "glove_path = 'glove.6B.50d.txt'  # Update this path\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "def create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim):\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in embeddings_dict:\n",
        "            embedding_matrix[ix] = embeddings_dict[word]\n",
        "        else:\n",
        "            embedding_matrix[ix] = torch.rand(embedding_dim)  # Random initialization for words not in GloVe\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cbade19",
      "metadata": {
        "id": "0cbade19"
      },
      "source": [
        "#### TO DO: Defining the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cKNdK6T7_OwQ",
      "metadata": {
        "id": "cKNdK6T7_OwQ"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: RNNLanguageModel()\n",
        "#######################################\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        \"\"\"\n",
        "        RNN language model con GRU y embeddings GloVe.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\n",
        "            \"mps\" if torch.backends.mps.is_available()\n",
        "            else \"cuda\" if torch.cuda.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Embedding inicializado con GloVe\n",
        "        # embedding_matrix: torch.Tensor [vocab_size, embedding_dim]\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight.copy_(embedding_matrix)\n",
        "        self.embedding.weight.requires_grad = False  # congelar embeddings\n",
        "\n",
        "        # GRU unidireccional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
        "                          hidden_size=hidden_dim,\n",
        "                          num_layers=1,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Capa final a vocab\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        x: [B, T] índices\n",
        "        hidden: [1, B, H] opcional\n",
        "        retorna: logits [B, T, V], hidden\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        if hidden is not None:\n",
        "            hidden = hidden.to(self.device)\n",
        "\n",
        "        emb = self.embedding(x)            # [B, T, D]\n",
        "        emb = self.dropout(emb)\n",
        "        out, hidden = self.rnn(emb, hidden)  # out: [B, T, H]\n",
        "        out = self.layer_norm(out)  \n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)              # [B, T, V]\n",
        "        return logits, hidden\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
        "        \"\"\"\n",
        "        Autoregresivo desde la secuencia dada.\n",
        "        Usa último token como condición y mantiene el hidden.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # tokens iniciales\n",
        "        tokens = sequence.strip().split()\n",
        "        # map a ids con UNK si no está\n",
        "        unk = UNK if 'UNK' in globals() else '<unk>'\n",
        "        start_ids = [word_to_ix.get(w, word_to_ix.get(unk, 0)) for w in tokens]\n",
        "        if len(start_ids) == 0:\n",
        "            # si vacío, inicia con <s> si existe\n",
        "            start_ids = [word_to_ix.get(START, 0)]\n",
        "\n",
        "        # construir estado inicial ejecutando la secuencia\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=self.device).unsqueeze(0)  # [1, T]\n",
        "        logits, hidden = self.forward(x)  # oculto después de la secuencia\n",
        "\n",
        "        generated = []\n",
        "        last_id = x[0, -1].unsqueeze(0).unsqueeze(0)  # [1,1]\n",
        "\n",
        "        for _ in range(num_words):\n",
        "            logits, hidden = self.forward(last_id, hidden)  # [1,1,V]\n",
        "            probs = torch.softmax(logits[0, -1], dim=-1)\n",
        "\n",
        "            if mode == 'multinomial':\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            wid = next_id.item()\n",
        "            word = ix_to_word.get(wid, unk)\n",
        "            generated.append(word)\n",
        "\n",
        "            # parar si EOS\n",
        "            if word == EOS:\n",
        "                break\n",
        "\n",
        "            last_id = next_id.view(1, 1)\n",
        "\n",
        "        return generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6c516d",
      "metadata": {
        "id": "fe6c516d"
      },
      "source": [
        "#### Training the Model\n",
        "The following code snippet provided is responsible for training the RNN language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "96135209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96135209",
        "outputId": "e1c60252-28fd-4c3a-a3f9-64c44122c408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 3.93922800560455, Perplexity: 51.378921746668304\n",
            "Epoch 2/50, Loss: 3.634114541187875, Perplexity: 37.868307221347365\n",
            "Epoch 3/50, Loss: 3.574944833012208, Perplexity: 35.69265170526054\n",
            "Epoch 4/50, Loss: 3.5450234194314785, Perplexity: 34.640496593497645\n",
            "Epoch 5/50, Loss: 3.523637490110477, Perplexity: 33.90754276605612\n",
            "Epoch 6/50, Loss: 3.509207688482624, Perplexity: 33.42177683207238\n",
            "Epoch 7/50, Loss: 3.494498620317437, Perplexity: 32.93377148894612\n",
            "Epoch 8/50, Loss: 3.4836063036604847, Perplexity: 32.57699302062646\n",
            "Epoch 9/50, Loss: 3.476397206437741, Perplexity: 32.342986810936445\n",
            "Epoch 10/50, Loss: 3.470231407952658, Perplexity: 32.144180003049264\n",
            "Epoch 11/50, Loss: 3.4652247427151623, Perplexity: 31.983647057135837\n",
            "Epoch 12/50, Loss: 3.4590095724175787, Perplexity: 31.785479703768175\n",
            "Epoch 13/50, Loss: 3.4562750355419523, Perplexity: 31.69867987017016\n",
            "Epoch 14/50, Loss: 3.4535726746328606, Perplexity: 31.61313423646732\n",
            "Epoch 15/50, Loss: 3.449323920436473, Perplexity: 31.47910273479876\n",
            "Epoch 16/50, Loss: 3.4471670921920516, Perplexity: 31.411280883245567\n",
            "Epoch 17/50, Loss: 3.443292755417399, Perplexity: 31.289818447705176\n",
            "Epoch 18/50, Loss: 3.442112166267833, Perplexity: 31.252899824704865\n",
            "Epoch 19/50, Loss: 3.4405596749242333, Perplexity: 31.204417612110078\n",
            "Epoch 20/50, Loss: 3.4387471388690205, Perplexity: 31.147909706884413\n",
            "Epoch 21/50, Loss: 3.4363841215241444, Perplexity: 31.074393550175728\n",
            "Epoch 22/50, Loss: 3.4358206570823655, Perplexity: 31.05688916637241\n",
            "Epoch 23/50, Loss: 3.434738487709132, Perplexity: 31.023298530745347\n",
            "Epoch 24/50, Loss: 3.4320217027294473, Perplexity: 30.93912928591733\n",
            "Epoch 25/50, Loss: 3.4308256436173625, Perplexity: 30.902146379694777\n",
            "Epoch 26/50, Loss: 3.428837596976123, Perplexity: 30.8407724986607\n",
            "Epoch 27/50, Loss: 3.4288666730378035, Perplexity: 30.841669239900938\n",
            "Epoch 28/50, Loss: 3.4285902432008104, Perplexity: 30.833144860551037\n",
            "Epoch 29/50, Loss: 3.4274158293878627, Perplexity: 30.79695524428234\n",
            "Epoch 30/50, Loss: 3.4260549369541207, Perplexity: 30.755072406390717\n",
            "Epoch 31/50, Loss: 3.4243609351707573, Perplexity: 30.70301736200411\n",
            "Epoch 32/50, Loss: 3.4242218757544554, Perplexity: 30.69874811517741\n",
            "Epoch 33/50, Loss: 3.4233945417343983, Perplexity: 30.673360499955553\n",
            "Epoch 34/50, Loss: 3.422657403455511, Perplexity: 30.650758323277177\n",
            "Epoch 35/50, Loss: 3.42138742071582, Perplexity: 30.611857096420025\n",
            "Epoch 36/50, Loss: 3.419721281984274, Perplexity: 30.560895961616463\n",
            "Epoch 37/50, Loss: 3.420321527752802, Perplexity: 30.579245516670287\n",
            "Epoch 38/50, Loss: 3.4196355573141712, Perplexity: 30.558276251180615\n",
            "Epoch 39/50, Loss: 3.419929047203165, Perplexity: 30.567246112502836\n",
            "Epoch 40/50, Loss: 3.419178959416721, Perplexity: 30.544326591429627\n",
            "Epoch 41/50, Loss: 3.419218929535534, Perplexity: 30.545547476191853\n",
            "Epoch 42/50, Loss: 3.4170401926765814, Perplexity: 30.47906921164833\n",
            "Epoch 43/50, Loss: 3.4175009340423053, Perplexity: 30.49311541520792\n",
            "Epoch 44/50, Loss: 3.416056646491445, Perplexity: 30.449106376730104\n",
            "Epoch 45/50, Loss: 3.4165186081535177, Perplexity: 30.463175946071132\n",
            "Epoch 46/50, Loss: 3.4166358749688204, Perplexity: 30.466748475163794\n",
            "Epoch 47/50, Loss: 3.416886786782844, Perplexity: 30.474393901414874\n",
            "Epoch 48/50, Loss: 3.4165841116127615, Perplexity: 30.465171454830802\n",
            "Epoch 49/50, Loss: 3.4150200047152577, Perplexity: 30.417557916063796\n",
            "Epoch 50/50, Loss: 3.4151580281041096, Perplexity: 30.421756540235474\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: RNNLanguageModel() and training\n",
        "#######################################\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 50\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.001)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    RNN.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(RNN.device)\n",
        "        targets = targets.to(RNN.device)\n",
        "\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Perplexity: {np.exp(avg_loss)}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0a980c45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting sequence: 'I love to'\n",
            "Mode max: \tI love to the sky </s>\n",
            "Mode multinomial: \tI love to be </s>\n",
            "\n",
            "Starting sequence: 'The sun is'\n",
            "Mode max: \tThe sun is </s>\n",
            "Mode multinomial: \tThe sun is something you want do </s>\n",
            "\n",
            "Starting sequence: 'When I was'\n",
            "Mode max: \tWhen I was </s>\n",
            "Mode multinomial: \tWhen I was flicking </s>\n",
            "\n",
            "Starting sequence: 'She said'\n",
            "Mode max: \tShe said </s>\n",
            "Mode multinomial: \tShe said now s now to my have i </s>\n",
            "\n",
            "Starting sequence: ''\n",
            "Mode max: \tand i m just in the way </s>\n",
            "Mode multinomial: \tman taking in i take me </s>\n"
          ]
        }
      ],
      "source": [
        "# Set the model to evaluation mode\n",
        "RNN.eval()\n",
        "\n",
        "# Example starting sequences\n",
        "start_sequences = [\n",
        "    \"I love to\",\n",
        "    \"The sun is\",\n",
        "    \"When I was\",\n",
        "    \"She said\",\n",
        "    \"\"  # Empty string (<s> token)\n",
        "]\n",
        "\n",
        "for sequence in start_sequences:\n",
        "    print(f\"\\nStarting sequence: '{sequence}'\")\n",
        "    \n",
        "    # Generate with greedy decoding (max probability)\n",
        "    generated_max = RNN.generate_sentence(\n",
        "        sequence=sequence,\n",
        "        word_to_ix=word_to_ix,\n",
        "        ix_to_word=ix_to_word,\n",
        "        num_words=15,\n",
        "        mode='max'\n",
        "    )\n",
        "    \n",
        "    # Generate with sampling (multinomial)\n",
        "    generated_sample = RNN.generate_sentence(\n",
        "        sequence=sequence,\n",
        "        word_to_ix=word_to_ix,\n",
        "        ix_to_word=ix_to_word,\n",
        "        num_words=15,\n",
        "        mode='multinomial'\n",
        "    )\n",
        "    \n",
        "    # Combine starting sequence with generated words\n",
        "    if sequence.strip():\n",
        "        full_text_max = sequence + \" \" + \" \".join(generated_max)\n",
        "        full_text_sample = sequence + \" \" + \" \".join(generated_sample)\n",
        "    else:\n",
        "        full_text_max = \" \".join(generated_max)\n",
        "        full_text_sample = \" \".join(generated_sample)\n",
        "    \n",
        "    print(f\"Mode max: \\t{full_text_max}\")\n",
        "    print(f\"Mode multinomial: \\t{full_text_sample}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b640e2b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
