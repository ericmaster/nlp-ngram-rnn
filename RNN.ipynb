{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# Language Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language models powered by n-grams and RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEQByNzNI4d",
        "outputId": "0a02ebec-4eab-4e46-c183-2849c72630bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "replace data/bbc/business.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "# !unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fda0b2",
      "metadata": {
        "id": "06fda0b2"
      },
      "source": [
        "### Step 2: RNN Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454617c7",
      "metadata": {
        "id": "454617c7"
      },
      "source": [
        "#### Preparing the Data\n",
        "The following Python code is used for loading and processing [GloVe (Global Vectors for Word Representation) embeddings](https://nlp.stanford.edu/projects/glove/). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. These embeddings can be used in various natural language processing and machine learning tasks.\n",
        "\n",
        "The `load_glove_embeddings(path)` function is used to load the GloVe embeddings from a file. The function takes a file path as an argument, reads the file line by line, and for each line, it splits the line into words and their corresponding embeddings, and stores them in a dictionary. The dictionary, embeddings_dict, maps words to their corresponding vector representations.\n",
        "\n",
        "The `create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim)` function is used to create an embedding matrix from the loaded GloVe embeddings. This function takes a dictionary mapping words to their indices (`word_to_ix`), the dictionary of GloVe embeddings (`embeddings_dict`), and the dimension of the embeddings (`embedding_dim`) as arguments. It creates a zero matrix of size (vocab_size, embedding_dim) and then for each word in  `word_to_ix`, it checks if the word is in `embeddings_dict`. If it is, it assigns the corresponding GloVe vector to the word's index in the embedding matrix. If the word is not in the embeddings_dict, it assigns a random vector to the word's index in the embedding matrix.\n",
        "\n",
        "The `glove_path` variable is the path to the GloVe file, and `glove_embeddings` is the dictionary of GloVe embeddings loaded using the `load_glove_embeddings` function. The `embedding_dim` variable is the dimension of the embeddings, and `embedding_matrix` is the embedding matrix created using the create_embedding_matrix function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a4411999",
      "metadata": {
        "id": "a4411999"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/taylor_swift.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aQ3a37oj1-F6",
      "metadata": {
        "id": "aQ3a37oj1-F6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9e9b6d54",
      "metadata": {
        "id": "9e9b6d54"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Path to the GloVe file\n",
        "glove_path = 'glove.6B.50d.txt'  # Update this path\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "def create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim):\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in embeddings_dict:\n",
        "            embedding_matrix[ix] = embeddings_dict[word]\n",
        "        else:\n",
        "            embedding_matrix[ix] = torch.rand(embedding_dim)  # Random initialization for words not in GloVe\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cbade19",
      "metadata": {
        "id": "0cbade19"
      },
      "source": [
        "#### TO DO: Defining the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cKNdK6T7_OwQ",
      "metadata": {
        "id": "cKNdK6T7_OwQ"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: RNNLanguageModel()\n",
        "#######################################\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        \"\"\"\n",
        "        RNN language model con GRU y embeddings GloVe.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\n",
        "            \"mps\" if torch.backends.mps.is_available()\n",
        "            else \"cuda\" if torch.cuda.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Embedding inicializado con GloVe\n",
        "        # embedding_matrix: torch.Tensor [vocab_size, embedding_dim]\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight.copy_(embedding_matrix)\n",
        "\n",
        "        # GRU unidireccional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
        "                          hidden_size=hidden_dim,\n",
        "                          num_layers=1,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # Capa final a vocab\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        x: [B, T] índices\n",
        "        hidden: [1, B, H] opcional\n",
        "        retorna: logits [B, T, V], hidden\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        if hidden is not None:\n",
        "            hidden = hidden.to(self.device)\n",
        "\n",
        "        emb = self.embedding(x)            # [B, T, D]\n",
        "        out, hidden = self.rnn(emb, hidden)  # out: [B, T, H]\n",
        "        logits = self.fc(out)              # [B, T, V]\n",
        "        return logits, hidden\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
        "        \"\"\"\n",
        "        Autoregresivo desde la secuencia dada.\n",
        "        Usa último token como condición y mantiene el hidden.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # tokens iniciales\n",
        "        tokens = sequence.strip().split()\n",
        "        # map a ids con UNK si no está\n",
        "        unk = UNK if 'UNK' in globals() else '<unk>'\n",
        "        start_ids = [word_to_ix.get(w, word_to_ix.get(unk, 0)) for w in tokens]\n",
        "        if len(start_ids) == 0:\n",
        "            # si vacío, inicia con <s> si existe\n",
        "            start_ids = [word_to_ix.get(START, 0)]\n",
        "\n",
        "        # construir estado inicial ejecutando la secuencia\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=self.device).unsqueeze(0)  # [1, T]\n",
        "        logits, hidden = self.forward(x)  # oculto después de la secuencia\n",
        "\n",
        "        generated = []\n",
        "        last_id = x[0, -1].unsqueeze(0).unsqueeze(0)  # [1,1]\n",
        "\n",
        "        for _ in range(num_words):\n",
        "            logits, hidden = self.forward(last_id, hidden)  # [1,1,V]\n",
        "            probs = torch.softmax(logits[0, -1], dim=-1)\n",
        "\n",
        "            if mode == 'multinomial':\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            wid = next_id.item()\n",
        "            word = ix_to_word.get(wid, unk)\n",
        "            generated.append(word)\n",
        "\n",
        "            # parar si EOS\n",
        "            if word == EOS:\n",
        "                break\n",
        "\n",
        "            last_id = next_id.view(1, 1)\n",
        "\n",
        "        return generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6c516d",
      "metadata": {
        "id": "fe6c516d"
      },
      "source": [
        "#### Training the Model\n",
        "The following code snippet provided is responsible for training the RNN language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "96135209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96135209",
        "outputId": "e1c60252-28fd-4c3a-a3f9-64c44122c408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/20, Loss: 1.8877289295196533, Perplexity: 6.60435268587878\n",
            "Epoch 2/20, Loss: 2.3066248893737793, Perplexity: 10.040479673564507\n",
            "Epoch 3/20, Loss: 1.6212486028671265, Perplexity: 5.05940356016781\n",
            "Epoch 4/20, Loss: 3.0175232887268066, Perplexity: 20.440603467026385\n",
            "Epoch 5/20, Loss: 1.1223251819610596, Perplexity: 3.0719888384478238\n",
            "Epoch 6/20, Loss: 2.61008882522583, Perplexity: 13.60025884424322\n",
            "Epoch 7/20, Loss: 1.8380917310714722, Perplexity: 6.284534229808462\n",
            "Epoch 8/20, Loss: 1.6776500940322876, Perplexity: 5.352962221983752\n",
            "Epoch 9/20, Loss: 2.4872968196868896, Perplexity: 12.028716343585533\n",
            "Epoch 10/20, Loss: 2.544201612472534, Perplexity: 12.733058112032381\n",
            "Epoch 11/20, Loss: 1.3362374305725098, Perplexity: 3.8047010881214876\n",
            "Epoch 12/20, Loss: 2.1243982315063477, Perplexity: 8.367860457933368\n",
            "Epoch 13/20, Loss: 2.313692331314087, Perplexity: 10.111691527118902\n",
            "Epoch 14/20, Loss: 2.348663330078125, Perplexity: 10.47156334210341\n",
            "Epoch 15/20, Loss: 2.6359918117523193, Perplexity: 13.957148464550283\n",
            "Epoch 16/20, Loss: 2.8863821029663086, Perplexity: 17.928329277056434\n",
            "Epoch 17/20, Loss: 2.5999062061309814, Perplexity: 13.462475278140287\n",
            "Epoch 18/20, Loss: 2.256592035293579, Perplexity: 9.550485924010976\n",
            "Epoch 19/20, Loss: 2.825664758682251, Perplexity: 16.872157171945545\n",
            "Epoch 20/20, Loss: 1.940276861190796, Perplexity: 6.960677845443573\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: RNNLanguageModel() and training\n",
        "#######################################\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 20\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(RNN.device)\n",
        "        targets = targets.to(RNN.device)\n",
        "\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
