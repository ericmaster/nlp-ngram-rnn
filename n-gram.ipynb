{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# N-gram Language Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language models powered by n-grams and RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEQByNzNI4d",
        "outputId": "4c3082db-6ed8-48b3-f8c3-91a3ec1cb82c"
      },
      "outputs": [],
      "source": [
        "# !unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f84134d",
      "metadata": {
        "id": "1f84134d"
      },
      "source": [
        "## Part 1: Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3c6b",
      "metadata": {
        "id": "4d0b3c6b"
      },
      "source": [
        "### Step 0: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "30667bf4",
      "metadata": {
        "id": "30667bf4"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install requests\n",
        "# !pip install torch\n",
        "# !pip install tqdm\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a6eedf",
      "metadata": {
        "id": "b3a6eedf"
      },
      "source": [
        "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "195db6a3",
      "metadata": {
        "id": "195db6a3"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ea47b0",
      "metadata": {
        "id": "f4ea47b0"
      },
      "source": [
        "We have performed a round of preprocessing on the datasets.\n",
        "\n",
        "- Each file contains one sentence per line.\n",
        "- All punctuation marks have been removed.\n",
        "- Each line is a sequences of tokens separated by whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbe98cc",
      "metadata": {
        "id": "5fbe98cc"
      },
      "source": [
        "#### Special Symbols ( Already defined in `utils.py` )\n",
        "The start and end tokens will act as padding to the given sentences, to make sure they are correctly defined, print them here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9ed9e54f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ed9e54f",
        "outputId": "c8530f18-f50a-49f3-e7b9-9a8f2a8158d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence START symbol: <s>\n",
            "Sentence END symbol: </s>\n",
            "Unknown word symbol: <UNK>\n"
          ]
        }
      ],
      "source": [
        "print(\"Sentence START symbol: {}\".format(START))\n",
        "print(\"Sentence END symbol: {}\".format(EOS))\n",
        "print(\"Unknown word symbol: {}\".format(UNK))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f4a6f",
      "metadata": {
        "id": "6b1f4a6f"
      },
      "source": [
        "#### Reading and processing an example file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d60ce7c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60ce7c2",
        "outputId": "6cb9715b-f965-49ac-e32b-c9beb36a5394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We are never ever ever ever ever getting back together\\n', 'We are the ones together we are back']\n"
          ]
        }
      ],
      "source": [
        "# Read the sample file\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4ec373cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ec373cc",
        "outputId": "9338a910-c2f0-4e07-babe-6cf398482b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>']\n",
            "['<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the content to add corresponding number of start and end tokens. Try out the method with n = 3 and n = 4 as well.\n",
        "# Preprocessing example for bigrams (n=2)\n",
        "sample = preprocess(sample, n=3)\n",
        "for s in sample:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "36a2a96e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36a2a96e",
        "outputId": "357866c7-11f8-4a9e-8459-852e05c1e6c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Flattens a nested list into a 1D list.\n",
        "flattened = flatten(sample)\n",
        "print(flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165b404",
      "metadata": {
        "id": "9165b404"
      },
      "source": [
        "### Step 1: N-Gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f73b0",
      "metadata": {
        "id": "5a4f73b0"
      },
      "source": [
        "#### TO DO: Defining `get_ngrams()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "138c35b6",
      "metadata": {
        "id": "138c35b6"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "\n",
        "    Returns:\n",
        "        n_grams: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "\n",
        "    return [\n",
        "        tuple(list_of_words[i : i + n]) for i in range(len(list_of_words) - n + 1)\n",
        "    ]\n",
        "\n",
        "    # raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5fdab35a",
      "metadata": {
        "id": "5fdab35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<s>', '<s>', 'we'), ('<s>', 'we', 'are'), ('we', 'are', 'never'), ('are', 'never', 'ever'), ('never', 'ever', 'ever'), ('ever', 'ever', 'ever'), ('ever', 'ever', 'ever'), ('ever', 'ever', 'getting'), ('ever', 'getting', 'back'), ('getting', 'back', 'together'), ('back', 'together', '</s>'), ('together', '</s>', '<s>'), ('</s>', '<s>', '<s>'), ('<s>', '<s>', 'we'), ('<s>', 'we', 'are'), ('we', 'are', 'the'), ('are', 'the', 'ones'), ('the', 'ones', 'together'), ('ones', 'together', 'we'), ('together', 'we', 'are'), ('we', 'are', 'back'), ('are', 'back', '</s>')]\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: get_ngrams()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=3)\n",
        "flattened = flatten(sample)\n",
        "\n",
        "print(get_ngrams(flattened, 3))\n",
        "assert get_ngrams(flattened, 3) == [('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'never'),\n",
        "        ('are', 'never', 'ever'),\n",
        "        ('never', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'getting'),\n",
        "        ('ever', 'getting', 'back'),\n",
        "        ('getting', 'back', 'together'),\n",
        "        ('back', 'together', '</s>'),\n",
        "        ('together', '</s>', '<s>'),\n",
        "        ('</s>', '<s>', '<s>'),\n",
        "        ('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'the'),\n",
        "        ('are', 'the', 'ones'),\n",
        "        ('the', 'ones', 'together'),\n",
        "        ('ones', 'together', 'we'),\n",
        "        ('together', 'we', 'are'),\n",
        "        ('we', 'are', 'back'),\n",
        "        ('are', 'back', '</s>')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bbb4a88",
      "metadata": {
        "id": "7bbb4a88"
      },
      "source": [
        "#### Class `NGramLanguageModel()`\n",
        "\n",
        "*Now*, we will define our LanguageModel class.\n",
        "\n",
        "**Some Useful Variables:**\n",
        "- self.model: `dict` of n-grams and their corresponding probabilities, keys being the tuple containing the n-gram, and the value being the probability of the n-gram.\n",
        "- self.vocab: `dict` of unigram vocabulary with counts, keys being the words themselves and the values being their frequency.\n",
        "- self.n: `int` value for n-gram order (e.g. 1, 2, 3).\n",
        "- self.train_data: `List[List]` containing preprocessed **unflattened** train sentences. You will have to flatten it to use in the language model\n",
        "- self.smoothing: `float` flag signifying the smoothing parameter.\n",
        "\n",
        "Note that we will not be using log probabilities in this section. Store the probabilities as they are, not in log space.\n",
        "\n",
        "**Laplace Smoothing**\n",
        "\n",
        "There are two ways to perform this:\n",
        "- Either you calculate all possible n-grams at train time and calculate smooth probabilities for all of them, hence inflating the model (eager emoothing). You then use the probabilities as when required at test time. **OR**\n",
        "- You calculate the probabilities for the **observed n-grams** at train time, using the smoothed likelihood formula, then if any unseen n-gram is observed at test time, you calculate the probability using the smoothed likelihood formula and store it in the model for future use (lazy smoothing).\n",
        "\n",
        "You will be implementing lazy smoothing\n",
        "\n",
        "**Perplexity**\n",
        "\n",
        "Steps:\n",
        "1. Flatten the test data.\n",
        "2. Extract ngrams from the flattened data.\n",
        "3. Calculate perplexity according to given formula. For unseen n-grams, calculate using smoothed likelihood and store the unseen n-gram probability in the labguage model `model` attribute:\n",
        "\n",
        "$ppl(W_{test}) = ppl(W_1W_2 ... W_n)^{-1/n} $\n",
        "\n",
        "Tips:\n",
        "- Remember that product changes to summation under `log`. Take the log of probabilities, sum them up, and then exponentiate it to get back to the original scale.\n",
        "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "24f2ce5c",
      "metadata": {
        "id": "24f2ce5c"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: NGramLanguageModel()\n",
        "#######################################\n",
        "class NGramLanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Language model class.\n",
        "\n",
        "        Args\n",
        "        ____\n",
        "        n: int\n",
        "            n-gram order\n",
        "        train_data: List[List]\n",
        "            already preprocessed unflattened list of sentences. e.g. [[\"<s>\", \"hello\", \"my\", \"</s>\"], [\"<s>\", \"hi\", \"there\", \"</s>\"]]\n",
        "        alpha: float\n",
        "            Smoothing parameter\n",
        "\n",
        "        Other attributes:\n",
        "            self.tokens: list of individual tokens present in the training corpus\n",
        "            self.vocab: vocabulary dict with counts\n",
        "            self.model: n-gram language model, i.e., n-gram dict with probabilties\n",
        "            self.n_grams_counts: dictionary for storing the frequency of ngrams in the training data, keys being the tuple of words(n-grams) and value being their frequency\n",
        "            self.prefix_counts: dictionary for storing the frequency of the (n-1) grams in the data, similar to the self.n_grams_counts\n",
        "            As an example:\n",
        "            For a trigram model, the n-gram would be (w1,w2,w3), the corresponding [n-1] gram would be (w1,w2)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.alpha = alpha\n",
        "        self.train_data = train_data\n",
        "        self.tokens = flatten(train_data)\n",
        "        self.vocab = Counter(self.tokens)\n",
        "        self.model = {}\n",
        "        self.n_grams_counts = {}\n",
        "        self.prefix_counts = {}\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Returns a n-gram dict with their smoothed probabilities. Remember to consider the edge case of n=1 as well\n",
        "\n",
        "        You are expected to update the self.n_grams_counts and self.prefix_counts, and use those calculate the probabilities.\n",
        "        \"\"\"\n",
        "        # Calculate n-grams counts\n",
        "        n_grams = get_ngrams(self.tokens, self.n)\n",
        "        self.n_grams_counts = Counter(n_grams)\n",
        "        \n",
        "        # Calculate (n-1)-grams counts (prefix counts)\n",
        "        if self.n > 1:\n",
        "            # The following line extracts ALL the (n-1)-grams from the corpus. It breaks the test cases.\n",
        "            # n_minus_1_grams = get_ngrams(self.tokens, self.n - 1)\n",
        "            # Iterates over n-grams and extracts the prefix (n-1)-gram by removing the last element\n",
        "            prefixes = [ngram[:-1] for ngram in n_grams]\n",
        "            self.prefix_counts = Counter(prefixes)\n",
        "        else:\n",
        "            # Handle the edge case of unigrams\n",
        "            self.prefix_counts = Counter({(): len(self.tokens)})\n",
        "        \n",
        "        N = len(self.vocab)\n",
        "        self.model = {}\n",
        "        for n_gram, count in self.n_grams_counts.items():\n",
        "            if self.n == 1:\n",
        "                # Handle the edge case of unigrams\n",
        "                prefix = ()\n",
        "            else:\n",
        "                prefix = n_gram[:-1]\n",
        "            prefix_count = self.prefix_counts[prefix]\n",
        "            prob = (count + self.alpha) / (prefix_count + self.alpha * N)\n",
        "            self.model[n_gram] = prob\n",
        "        return self.model\n",
        "\n",
        "        # raise NotImplementedError\n",
        "\n",
        "    def get_smooth_probabilities(self, ngrams):\n",
        "        \"\"\"\n",
        "        Returns the smoothed probability of the n-gram, using Laplace Smoothing.\n",
        "        Remember to consider the edge case of  n = 1\n",
        "        HINT: Use self.n_gram_counts, self.tokens and self.prefix_counts\n",
        "        \"\"\"\n",
        "        N = len(self.vocab)\n",
        "        probabilities = {}\n",
        "        for ngram in ngrams:\n",
        "            count = self.n_grams_counts.get(ngram, 0)\n",
        "            if self.n == 1:\n",
        "                # Handle the edge case of unigrams\n",
        "                prefix_count = len(self.tokens)\n",
        "            else:\n",
        "                prefix = ngram[:-1]\n",
        "                prefix_count = self.prefix_counts.get(prefix, 0)\n",
        "            prob = (count + self.alpha) / (prefix_count + self.alpha * N)\n",
        "            probabilities[ngram] = prob\n",
        "        return probabilities\n",
        "        # raise NotImplementedError\n",
        "\n",
        "    def get_prob(self, ngram):\n",
        "        \"\"\"\n",
        "        Returns the probability of the n-gram, using Laplace Smoothing.\n",
        "\n",
        "        Args\n",
        "        ____\n",
        "        ngram: tuple\n",
        "            n-gram tuple\n",
        "\n",
        "        Returns\n",
        "        _______\n",
        "        float\n",
        "            probability of the n-gram\n",
        "        \"\"\"\n",
        "        # Build the model if not already built\n",
        "        if self.model is None:\n",
        "            self.build()\n",
        "\n",
        "        # Hint: Check if this n-gram exists in self.model, if it does simply return it!\n",
        "        # Otherwise, calculate the probabillity similar to get_smooth_probabilities()\n",
        "        if ngram in self.model:\n",
        "            return self.model[ngram]\n",
        "        else:\n",
        "            N = len(self.vocab)\n",
        "            count = self.n_grams_counts.get(ngram, 0)\n",
        "            if self.n == 1:\n",
        "                # Handle the edge case of unigrams\n",
        "                prefix_count = len(self.tokens)\n",
        "            else:\n",
        "                prefix = ngram[:-1]\n",
        "                prefix_count = self.prefix_counts.get(prefix, 0)\n",
        "            \n",
        "            # Handle division by zero when alpha=0 and prefix_count=0\n",
        "            denominator = prefix_count + self.alpha * N\n",
        "            if denominator == 0:\n",
        "                prob = 0.0\n",
        "            else:\n",
        "                prob = (count + self.alpha) / denominator\n",
        "            \n",
        "            # Store the calculated probability for future use (lazy smoothing)\n",
        "            self.model[ngram] = prob\n",
        "            return prob\n",
        "        # raise NotImplementedError\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        \"\"\"\n",
        "        Returns perplexity calculated on the test data.\n",
        "        Args\n",
        "        ----------\n",
        "        test_data: List[List]\n",
        "            Already preprocessed nested list of sentences\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Calculated perplexity value\n",
        "        \"\"\"\n",
        "        # Flatten the test data\n",
        "        test_tokens = flatten(test_data)\n",
        "        \n",
        "        # Extract n-grams from the flattened data\n",
        "        test_ngrams = get_ngrams(test_tokens, self.n)\n",
        "        \n",
        "        # Calculate the log probability sum\n",
        "        log_prob_sum = 0.0\n",
        "        Epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "        N = len(test_ngrams) + Epsilon\n",
        "        \n",
        "        for ngram in test_ngrams:\n",
        "            prob = self.get_prob(ngram)\n",
        "            log_prob_sum += math.log(prob)\n",
        "        \n",
        "        # Calculate perplexity: exp(-1/(N + Epsilon) * sum(log(p(w_i))))\n",
        "        avg_log_prob = log_prob_sum / N\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "        \n",
        "        return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "909b9c4a",
      "metadata": {
        "id": "909b9c4a"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: NGramLanguageModel()\n",
        "#######################################\n",
        "# For the sake of understanding we will pass alpha as 0 (no smoothing), so that you gain intuition about the probabilities\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
        "\n",
        "expected_vocab = Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "expected_model = {('<s>', 'we'): 1.0,\n",
        "        ('we', 'are'): 1.0,\n",
        "        ('are', 'never'): 0.3333333333333333,\n",
        "        ('never', 'ever'): 1.0,\n",
        "        ('ever', 'ever'): 0.75,\n",
        "        ('ever', 'getting'): 0.25,\n",
        "        ('getting', 'back'): 1.0,\n",
        "        ('back', 'together'): 0.5,\n",
        "        ('together', '</s>'): 0.5,\n",
        "        ('</s>', '<s>'): 1.0,\n",
        "        ('are', 'the'): 0.3333333333333333,\n",
        "        ('the', 'ones'): 1.0,\n",
        "        ('ones', 'together'): 1.0,\n",
        "        ('together', 'we'): 0.5,\n",
        "        ('are', 'back'): 0.3333333333333333,\n",
        "        ('back', '</s>'): 0.5}\n",
        "\n",
        "assert test_lm.vocab == expected_vocab, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
        "\n",
        "assert test_lm.model == expected_model, (\n",
        "    f\"Model mismatch! \\n\"\n",
        "    f\"Expected keys but missing: {set(expected_model.keys()) - set(test_lm.model.keys())}\\n\"\n",
        "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model.keys())}\\n\"\n",
        "    f\"Discrepancies in probabilities: \"\n",
        "    f\"{ {k: (expected_model[k], test_lm.model[k]) for k in expected_model if k in test_lm.model and expected_model[k] != test_lm.model[k]} }\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "501ac225",
      "metadata": {
        "id": "501ac225"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST smoothing: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "\n",
        "expected_vocab_smoothing = Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "expected_model_smoothing ={('<s>', 'we'): 0.23076923076923078,\n",
        "        ('we', 'are'): 0.2857142857142857,\n",
        "        ('are', 'never'): 0.14285714285714285,\n",
        "        ('never', 'ever'): 0.16666666666666666,\n",
        "        ('ever', 'ever'): 0.26666666666666666,\n",
        "        ('ever', 'getting'): 0.13333333333333333,\n",
        "        ('getting', 'back'): 0.16666666666666666,\n",
        "        ('back', 'together'): 0.15384615384615385,\n",
        "        ('together', '</s>'): 0.15384615384615385,\n",
        "        ('</s>', '<s>'): 0.16666666666666666,\n",
        "        ('are', 'the'): 0.14285714285714285,\n",
        "        ('the', 'ones'): 0.16666666666666666,\n",
        "        ('ones', 'together'): 0.16666666666666666,\n",
        "        ('together', 'we'): 0.15384615384615385,\n",
        "        ('are', 'back'): 0.14285714285714285,\n",
        "        ('back', '</s>'): 0.15384615384615385}\n",
        "\n",
        "\n",
        "assert test_lm.vocab == expected_vocab_smoothing, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
        "\n",
        "assert test_lm.model == expected_model_smoothing, (\n",
        "    f\"Model mismatch! \\n\"\n",
        "    f\"Expected keys but missing: {set(expected_model_smoothing.keys()) - set(test_lm.model.keys())}\\n\"\n",
        "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model_smoothing.keys())}\\n\"\n",
        "    f\"Discrepancies in probabilities: \"\n",
        "    f\"{ {k: (expected_model_smoothing[k], test_lm.model[k]) for k in expected_model_smoothing if k in test_lm.model and expected_model_smoothing[k] != test_lm.model[k]} }\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "157b0749",
      "metadata": {
        "id": "157b0749"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST unigram: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=1)\n",
        "test_lm = NGramLanguageModel(n=1, train_data=sample, alpha=1)\n",
        "\n",
        "expected_vocab_unigram = Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "expected_model_unigram = {('<s>',): 0.09090909090909091,\n",
        "        ('we',): 0.12121212121212122,\n",
        "        ('are',): 0.12121212121212122,\n",
        "        ('never',): 0.06060606060606061,\n",
        "        ('ever',): 0.15151515151515152,\n",
        "        ('getting',): 0.06060606060606061,\n",
        "        ('back',): 0.09090909090909091,\n",
        "        ('together',): 0.09090909090909091,\n",
        "        ('</s>',): 0.09090909090909091,\n",
        "        ('the',): 0.06060606060606061,\n",
        "        ('ones',): 0.06060606060606061}\n",
        "\n",
        "\n",
        "assert test_lm.vocab == expected_vocab_unigram, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
        "\n",
        "assert test_lm.model == expected_model_unigram, (\n",
        "    f\"Model mismatch! \\n\"\n",
        "    f\"Expected keys but missing: {set(expected_model_unigram.keys()) - set(test_lm.model.keys())}\\n\"\n",
        "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model_unigram.keys())}\\n\"\n",
        "    f\"Discrepancies in probabilities: \"\n",
        "    f\"{ {k: (expected_model_unigram[k], test_lm.model[k]) for k in expected_model_unigram if k in test_lm.model and expected_model_unigram[k] != test_lm.model[k]} }\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2927e9aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2927e9aa",
        "outputId": "f6d802e3-2f72-4599-e24c-6bbb2730a644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2972789669785443\n",
            "5.283124177741067\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: perplexity()\n",
        "#######################################\n",
        "test_lm = NGramLanguageModel(n=3, train_data=sample, alpha=0)\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "print(test_ppl)\n",
        "assert test_ppl < 1.7\n",
        "assert test_ppl > 0\n",
        "\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "print(test_ppl)\n",
        "# assert test_ppl < 5.0\n",
        "assert test_ppl < 5.3\n",
        "assert test_ppl > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "76b7d788",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test tokens: ['<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n",
            "Test ngrams: [('<s>', 'we'), ('we', 'are'), ('are', 'never'), ('never', 'ever'), ('ever', 'ever'), ('ever', 'ever'), ('ever', 'ever'), ('ever', 'getting'), ('getting', 'back'), ('back', 'together'), ('together', '</s>'), ('</s>', '<s>'), ('<s>', 'we'), ('we', 'are'), ('are', 'the'), ('the', 'ones'), ('ones', 'together'), ('together', 'we'), ('we', 'are'), ('are', 'back'), ('back', '</s>')]\n",
            "Number of ngrams: 21\n",
            "P(('<s>', 'we')) = 0.23076923076923078\n",
            "P(('we', 'are')) = 0.2857142857142857\n",
            "P(('are', 'never')) = 0.14285714285714285\n",
            "P(('never', 'ever')) = 0.16666666666666666\n",
            "P(('ever', 'ever')) = 0.26666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Debug perplexity calculation\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "test_tokens = flatten(sample)\n",
        "test_ngrams = get_ngrams(test_tokens, 2)\n",
        "print(\"Test tokens:\", test_tokens)\n",
        "print(\"Test ngrams:\", test_ngrams)\n",
        "print(\"Number of ngrams:\", len(test_ngrams))\n",
        "\n",
        "# Check a few probabilities\n",
        "for ngram in test_ngrams[:5]:\n",
        "    prob = test_lm.get_prob(ngram)\n",
        "    print(f\"P({ngram}) = {prob}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VW7eglKVDiAe",
      "metadata": {
        "id": "VW7eglKVDiAe"
      },
      "source": [
        "## Train the n-gram language model on the data/bbc/business.txt dataset for n = 2 and n = 3. Then do the same for data/bbc/sports.txt datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "IIslb4CFDbHK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIslb4CFDbHK",
        "outputId": "fa120782-8806-48b5-8507-2d2f0d840e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83819\n",
            "83819\n",
            "Vocab size:  11916\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for business data\n",
        "#######################################\n",
        "business_prepro = preprocess(read_file(\"data/bbc/business.txt\"), n=2)\n",
        "train_bussi = NGramLanguageModel(n=2, train_data=business_prepro, alpha=0.5)\n",
        "print(len(set(train_bussi.model.keys())))\n",
        "print(len(train_bussi.n_grams_counts))\n",
        "print('Vocab size: ', len(train_bussi.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5WobtxwEDbC8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WobtxwEDbC8",
        "outputId": "2d7586cc-451b-4238-917c-40f63cba40f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141221\n",
            "141221\n",
            "Vocab size:  11916\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for business data\n",
        "#######################################\n",
        "business_prepro = preprocess(read_file(\"data/bbc/business.txt\"), n=3)\n",
        "train_bussi = NGramLanguageModel(n=3, train_data=business_prepro, alpha=0.5)\n",
        "print(len(set(train_bussi.model.keys())))\n",
        "print(len(train_bussi.n_grams_counts))\n",
        "print('Vocab size: ', len(train_bussi.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "UYlDBvvSDa97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYlDBvvSDa97",
        "outputId": "61664c5f-635c-418e-86b2-1ab9516cf114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77398\n",
            "77398\n",
            "Vocab size:  10607\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for sports data\n",
        "#######################################\n",
        "spo_prepro = preprocess(read_file(\"data/bbc/sport.txt\"), n=2)\n",
        "train_spo = NGramLanguageModel(n=2, train_data=spo_prepro, alpha=0.5)\n",
        "print(len(set(train_spo.model.keys())))\n",
        "print(len(train_spo.n_grams_counts))\n",
        "print('Vocab size: ', len(train_spo.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5Qyq4NM8Da0v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qyq4NM8Da0v",
        "outputId": "af748953-8404-4a4d-8c4c-71c76346a514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135645\n",
            "135645\n",
            "Vocab size:  10607\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for sports data\n",
        "#######################################\n",
        "spo_prepro = preprocess(read_file(\"data/bbc/sport.txt\"), n=3)\n",
        "train_spo = NGramLanguageModel(n=3, train_data=spo_prepro, alpha=0.5)\n",
        "print(len(set(train_spo.model.keys())))\n",
        "print(len(train_spo.n_grams_counts))\n",
        "print('Vocab size: ', len(train_spo.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zKE4iK0DGfZ_",
      "metadata": {
        "id": "zKE4iK0DGfZ_"
      },
      "source": [
        "How many possible 2- and 3- grams could there be, given the same vocabulary?\n",
        "\n",
        "\n",
        "How do the empirical counts given above compare to the number of possible 2- and 3- grams?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I90mZ5RkJsxs",
      "metadata": {
        "id": "I90mZ5RkJsxs"
      },
      "source": [
        "## Train a tri-gram (n=3, smoothing= 0.1) language models on collections of song lyrics from three popular artists (‘data/lyrics/‘) and use the model to score a new unattributed song."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "a_aIvsckJySF",
      "metadata": {
        "id": "a_aIvsckJySF"
      },
      "outputs": [],
      "source": [
        "taylor_pre = preprocess(read_file(\"data/lyrics/taylor_swift.txt\"), n=3)\n",
        "train_tay = NGramLanguageModel(n=3, train_data=taylor_pre, alpha=0.1)\n",
        "\n",
        "green_pre = preprocess(read_file(\"data/lyrics/green_day.txt\"), n=3)\n",
        "train_green = NGramLanguageModel(n=3, train_data=green_pre, alpha=0.1)\n",
        "\n",
        "ed_pre = preprocess(read_file(\"data/lyrics/ed_sheeran.txt\"), n=3)\n",
        "train_ed = NGramLanguageModel(n=3, train_data=ed_pre, alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fmju45LGKbi1",
      "metadata": {
        "id": "fmju45LGKbi1"
      },
      "source": [
        "What are the perplexity scores of the test lyrics against each of the language models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "Aw9AMywdJyK8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw9AMywdJyK8",
        "outputId": "81cb5df8-3608-4038-97cc-1323242abb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity of taylor swift:  138.00663307867177\n",
            "Perplexity of green day:  522.5401188671458\n",
            "Perplexity of ed sheeran:  521.2574891174802\n"
          ]
        }
      ],
      "source": [
        "test_prepro = preprocess(read_file(\"data/lyrics/test_lyrics.txt\"), n=3)\n",
        "\n",
        "tay_ppl = train_tay.perplexity(test_prepro)\n",
        "print('Perplexity of taylor swift: ', tay_ppl)\n",
        "\n",
        "green_ppl = train_green.perplexity(test_prepro)\n",
        "print('Perplexity of green day: ', green_ppl)\n",
        "\n",
        "ed_ppl = train_ed.perplexity(test_prepro)\n",
        "print('Perplexity of ed sheeran: ', ed_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUdAiTpgVRpv",
      "metadata": {
        "id": "ZUdAiTpgVRpv"
      },
      "source": [
        "## Train a bi-gram (n=2, smoothing= 0.1) language models on collections of song lyrics from three popular artists (‘data/lyrics/‘) and use the model to score a new unattributed song."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1iEOWaj_VMyA",
      "metadata": {
        "id": "1iEOWaj_VMyA"
      },
      "outputs": [],
      "source": [
        "taylor_pre = preprocess(read_file(\"data/lyrics/taylor_swift.txt\"), n=2)\n",
        "train_tay = NGramLanguageModel(n=2, train_data=taylor_pre, alpha=0.1)\n",
        "\n",
        "green_pre = preprocess(read_file(\"data/lyrics/green_day.txt\"), n=2)\n",
        "train_green = NGramLanguageModel(n=2, train_data=green_pre, alpha=0.1)\n",
        "\n",
        "ed_pre = preprocess(read_file(\"data/lyrics/ed_sheeran.txt\"), n=2)\n",
        "train_ed = NGramLanguageModel(n=2, train_data=ed_pre, alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "PLoaDiReVMtN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLoaDiReVMtN",
        "outputId": "718d8f09-fbec-4525-a136-ad5079b87fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity of taylor swift:  90.3650466186274\n",
            "Perplexity of green day:  286.3889893159217\n",
            "Perplexity of ed sheeran:  298.34788470321604\n"
          ]
        }
      ],
      "source": [
        "test_prepro = preprocess(read_file(\"data/lyrics/test_lyrics.txt\"), n=2)\n",
        "\n",
        "tay_ppl = train_tay.perplexity(test_prepro)\n",
        "print('Perplexity of taylor swift: ', tay_ppl)\n",
        "\n",
        "green_ppl = train_green.perplexity(test_prepro)\n",
        "print('Perplexity of green day: ', green_ppl)\n",
        "\n",
        "ed_ppl = train_ed.perplexity(test_prepro)\n",
        "print('Perplexity of ed sheeran: ', ed_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785c2cc7",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "En este notebook hemos implementado la clase `NGramLanguageModel` para el modelamiento de lenguaje a través de N-gramas con suavizado de Laplace, evaluando sobre un conjunto de datos de letras de canciones de varios artistas. Uno de los hallazgos sobre la implementación de los n-gramas fue que los trigramas (n=3) capturan más contexto pero sufren de mayor escasez de datos mientras los bigramas (n=2) son más robustos para texto no visto debido a su menor dependencia contextual. El suavizado de Laplace ayuda pero puede no ser óptimo para todos los casos\n",
        "\n",
        "En cuanto a la perplejidad, los experimentos con letras de canciones demuestran que cada artista tiene patrones lingüísticos únicos capturados por su modelo. Además, una menor perplejidad indica mayor similitud estilística. Los modelos tienen una utilidad suficiente para clasificación o validación de autoría. En cuanto a las observaciones de los Datos BBC, notamos que los vocabularios grandes generan muchos N-gramas posibles ($V^n$). Solo una fracción pequeña se observa en los datos reales lo que demuestra el problema de escasez fundamental en N-gramas.\n",
        "\n",
        "El modelo tiene algunas limitaciones como contexto limitado, es decir, considera solo dependencias locales para las predicciones. Además, existe una explosión combinatorial, es decir, $V^n$ posibles N-gramas. Laplace puede no siempre ser la mejor opción para el suavizado.\n",
        "\n",
        "El modelo, no obstante, tiene algunas aplicaciones prácticas. Por ejemplo, podemos utilizar para una detección de autoría y análisis estilístico. También podemos utililizar el modelo en sistemas de corrección automática o evaluación de coherencia textual. El modelo también puede servir como un baseline para comparación con modelos más complejos (RNNs, Transformers).\n",
        "\n",
        "Los N-gramas siguen siendo fundamentales para entender el procesamiento de lenguaje natural, proporcionando intuición sobre probabilidades de secuencias y la importancia del contexto en el lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7c3389",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
