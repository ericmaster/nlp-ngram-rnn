{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# Language Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language models powered by n-grams and RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEQByNzNI4d",
        "outputId": "0a02ebec-4eab-4e46-c183-2849c72630bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "replace data/bbc/business.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f84134d",
      "metadata": {
        "id": "1f84134d"
      },
      "source": [
        "## Part 1: Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3c6b",
      "metadata": {
        "id": "4d0b3c6b"
      },
      "source": [
        "### Step 0: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "30667bf4",
      "metadata": {
        "id": "30667bf4"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install requests\n",
        "# !pip install torch\n",
        "# !pip install tqdm\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a6eedf",
      "metadata": {
        "id": "b3a6eedf"
      },
      "source": [
        "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "195db6a3",
      "metadata": {
        "id": "195db6a3"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ea47b0",
      "metadata": {
        "id": "f4ea47b0"
      },
      "source": [
        "We have performed a round of preprocessing on the datasets.\n",
        "\n",
        "- Each file contains one sentence per line.\n",
        "- All punctuation marks have been removed.\n",
        "- Each line is a sequences of tokens separated by whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbe98cc",
      "metadata": {
        "id": "5fbe98cc"
      },
      "source": [
        "#### Special Symbols ( Already defined in `utils.py` )\n",
        "The start and end tokens will act as padding to the given sentences, to make sure they are correctly defined, print them here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ed9e54f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ed9e54f",
        "outputId": "ffd3bf37-fd39-4fab-9f5c-8dce5b21cc61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence START symbol: <s>\n",
            "Sentence END symbol: </s>\n",
            "Unknown word symbol: <UNK>\n"
          ]
        }
      ],
      "source": [
        "print(\"Sentence START symbol: {}\".format(START))\n",
        "print(\"Sentence END symbol: {}\".format(EOS))\n",
        "print(\"Unknown word symbol: {}\".format(UNK))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f4a6f",
      "metadata": {
        "id": "6b1f4a6f"
      },
      "source": [
        "#### Reading and processing an example file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d60ce7c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60ce7c2",
        "outputId": "d1290406-f9dc-454b-e077-242cc79429de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We are never ever ever ever ever getting back together\\n', 'We are the ones together we are back']\n"
          ]
        }
      ],
      "source": [
        "# Read the sample file\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ec373cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ec373cc",
        "outputId": "3db627b5-ce62-4a08-e1bc-7ff60fb917ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>']\n",
            "['<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the content to add corresponding number of start and end tokens. Try out the method with n = 3 and n = 4 as well.\n",
        "# Preprocessing example for bigrams (n=2)\n",
        "sample = preprocess(sample, n=3)\n",
        "for s in sample:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "36a2a96e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36a2a96e",
        "outputId": "9f542b69-e43f-4fa7-a9d3-58e7e93c9a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Flattens a nested list into a 1D list.\n",
        "flattened = flatten(sample)\n",
        "print(flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165b404",
      "metadata": {
        "id": "9165b404"
      },
      "source": [
        "### Step 1: N-Gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f73b0",
      "metadata": {
        "id": "5a4f73b0"
      },
      "source": [
        "#### TO DO: Defining `get_ngrams()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "138c35b6",
      "metadata": {
        "id": "138c35b6"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "\n",
        "    Returns:\n",
        "        n_grams: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5fdab35a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "5fdab35a",
        "outputId": "dd3f89ba-89de-4a04-ae37-f82dd70b4aab"
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1579726590.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mflattened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m assert get_ngrams(flattened, 3) == [('<s>', '<s>', 'we'),\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'<s>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'we'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'are'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'we'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'never'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3956189955.py\u001b[0m in \u001b[0;36mget_ngrams\u001b[0;34m(list_of_words, n)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: get_ngrams()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=3)\n",
        "flattened = flatten(sample)\n",
        "\n",
        "assert get_ngrams(flattened, 3) == [('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'never'),\n",
        "        ('are', 'never', 'ever'),\n",
        "        ('never', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'getting'),\n",
        "        ('ever', 'getting', 'back'),\n",
        "        ('getting', 'back', 'together'),\n",
        "        ('back', 'together', '</s>'),\n",
        "        ('together', '</s>', '<s>'),\n",
        "        ('</s>', '<s>', '<s>'),\n",
        "        ('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'the'),\n",
        "        ('are', 'the', 'ones'),\n",
        "        ('the', 'ones', 'together'),\n",
        "        ('ones', 'together', 'we'),\n",
        "        ('together', 'we', 'are'),\n",
        "        ('we', 'are', 'back'),\n",
        "        ('are', 'back', '</s>')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bbb4a88",
      "metadata": {
        "id": "7bbb4a88"
      },
      "source": [
        "#### **TO DO:** Class `NGramLanguageModel()`\n",
        "\n",
        "*Now*, we will define our LanguageModel class.\n",
        "\n",
        "**Some Useful Variables:**\n",
        "- self.model: `dict` of n-grams and their corresponding probabilities, keys being the tuple containing the n-gram, and the value being the probability of the n-gram.\n",
        "- self.vocab: `dict` of unigram vocabulary with counts, keys being the words themselves and the values being their frequency.\n",
        "- self.n: `int` value for n-gram order (e.g. 1, 2, 3).\n",
        "- self.train_data: `List[List]` containing preprocessed **unflattened** train sentences. You will have to flatten it to use in the language model\n",
        "- self.smoothing: `float` flag signifying the smoothing parameter.\n",
        "\n",
        "Note that we will not be using log probabilities in this section. Store the probabilities as they are, not in log space.\n",
        "\n",
        "**Laplace Smoothing**\n",
        "\n",
        "There are two ways to perform this:\n",
        "- Either you calculate all possible n-grams at train time and calculate smooth probabilities for all of them, hence inflating the model (eager emoothing). You then use the probabilities as when required at test time. **OR**\n",
        "- You calculate the probabilities for the **observed n-grams** at train time, using the smoothed likelihood formula, then if any unseen n-gram is observed at test time, you calculate the probability using the smoothed likelihood formula and store it in the model for future use (lazy smoothing).\n",
        "\n",
        "You will be implementing lazy smoothing\n",
        "\n",
        "**Perplexity**\n",
        "\n",
        "Steps:\n",
        "1. Flatten the test data.\n",
        "2. Extract ngrams from the flattened data.\n",
        "3. Calculate perplexity according to given formula. For unseen n-grams, calculate using smoothed likelihood and store the unseen n-gram probability in the labguage model `model` attribute:\n",
        "\n",
        "$ppl(W_{test}) = ppl(W_1W_2 ... W_n)^{-1/n} $\n",
        "\n",
        "Tips:\n",
        "- Remember that product changes to summation under `log`. Take the log of probabilities, sum them up, and then exponentiate it to get back to the original scale.\n",
        "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f2ce5c",
      "metadata": {
        "id": "24f2ce5c"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: NGramLanguageModel()\n",
        "#######################################\n",
        "class NGramLanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1):\n",
        "        \"\"\"\n",
        "        Language model class.\n",
        "\n",
        "        Args\n",
        "        ____\n",
        "        n: int\n",
        "            n-gram order\n",
        "        train_data: List[List]\n",
        "            already preprocessed unflattened list of sentences. e.g. [[\"<s>\", \"hello\", \"my\", \"</s>\"], [\"<s>\", \"hi\", \"there\", \"</s>\"]]\n",
        "        alpha: float\n",
        "            Smoothing parameter\n",
        "\n",
        "        Other attributes:\n",
        "            self.tokens: list of individual tokens present in the training corpus\n",
        "            self.vocab: vocabulary dict with counts\n",
        "            self.model: n-gram language model, i.e., n-gram dict with probabilties\n",
        "            self.n_grams_counts: dictionary for storing the frequency of ngrams in the training data, keys being the tuple of words(n-grams) and value being their frequency\n",
        "            self.prefix_counts: dictionary for storing the frequency of the (n-1) grams in the data, similar to the self.n_grams_counts\n",
        "            As an example:\n",
        "            For a trigram model, the n-gram would be (w1,w2,w3), the corresponding [n-1] gram would be (w1,w2)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Returns a n-gram dict with their smoothed probabilities. Remember to consider the edge case of n=1 as well\n",
        "\n",
        "        You are expected to update the self.n_grams_counts and self.prefix_counts, and use those calculate the probabilities.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_smooth_probabilities(self, ngrams):\n",
        "        \"\"\"\n",
        "        Returns the smoothed probability of the n-gram, using Laplace Smoothing.\n",
        "        Remember to consider the edge case of  n = 1\n",
        "        HINT: Use self.n_gram_counts, self.tokens and self.prefix_counts\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_prob(self, ngram):\n",
        "        \"\"\"\n",
        "        Returns the probability of the n-gram, using Laplace Smoothing.\n",
        "\n",
        "        Args\n",
        "        ____\n",
        "        ngram: tuple\n",
        "            n-gram tuple\n",
        "\n",
        "        Returns\n",
        "        _______\n",
        "        float\n",
        "            probability of the n-gram\n",
        "        \"\"\"\n",
        "\n",
        "        # Hint: Check if this n-gram exists in self.model, if it does simply return it!\n",
        "        # Otherwise, calculate the probabillity similar to get_smooth_probabilities()\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        \"\"\"\n",
        "        Returns perplexity calculated on the test data.\n",
        "        Args\n",
        "        ----------\n",
        "        test_data: List[List]\n",
        "            Already preprocessed nested list of sentences\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Calculated perplexity value\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909b9c4a",
      "metadata": {
        "id": "909b9c4a"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: NGramLanguageModel()\n",
        "#######################################\n",
        "# For the sake of understanding we will pass alpha as 0 (no smoothing), so that you gain intuition about the probabilities\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
        "\n",
        "expected_vocab = Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "expected_model = {('<s>', 'we'): 1.0,\n",
        "        ('we', 'are'): 1.0,\n",
        "        ('are', 'never'): 0.3333333333333333,\n",
        "        ('never', 'ever'): 1.0,\n",
        "        ('ever', 'ever'): 0.75,\n",
        "        ('ever', 'getting'): 0.25,\n",
        "        ('getting', 'back'): 1.0,\n",
        "        ('back', 'together'): 0.5,\n",
        "        ('together', '</s>'): 0.5,\n",
        "        ('</s>', '<s>'): 1.0,\n",
        "        ('are', 'the'): 0.3333333333333333,\n",
        "        ('the', 'ones'): 1.0,\n",
        "        ('ones', 'together'): 1.0,\n",
        "        ('together', 'we'): 0.5,\n",
        "        ('are', 'back'): 0.3333333333333333,\n",
        "        ('back', '</s>'): 0.5}\n",
        "\n",
        "assert test_lm.vocab == expected_vocab, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
        "\n",
        "assert test_lm.model == expected_model, (\n",
        "    f\"Model mismatch! \\n\"\n",
        "    f\"Expected keys but missing: {set(expected_model.keys()) - set(test_lm.model.keys())}\\n\"\n",
        "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model.keys())}\\n\"\n",
        "    f\"Discrepancies in probabilities: \"\n",
        "    f\"{ {k: (expected_model[k], test_lm.model[k]) for k in expected_model if k in test_lm.model and expected_model[k] != test_lm.model[k]} }\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501ac225",
      "metadata": {
        "id": "501ac225"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST smoothing: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "\n",
        "expected_vocab_smoothing = Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "expected_model_smoothing ={('<s>', 'we'): 0.23076923076923078,\n",
        "        ('we', 'are'): 0.2857142857142857,\n",
        "        ('are', 'never'): 0.14285714285714285,\n",
        "        ('never', 'ever'): 0.16666666666666666,\n",
        "        ('ever', 'ever'): 0.26666666666666666,\n",
        "        ('ever', 'getting'): 0.13333333333333333,\n",
        "        ('getting', 'back'): 0.16666666666666666,\n",
        "        ('back', 'together'): 0.15384615384615385,\n",
        "        ('together', '</s>'): 0.15384615384615385,\n",
        "        ('</s>', '<s>'): 0.16666666666666666,\n",
        "        ('are', 'the'): 0.14285714285714285,\n",
        "        ('the', 'ones'): 0.16666666666666666,\n",
        "        ('ones', 'together'): 0.16666666666666666,\n",
        "        ('together', 'we'): 0.15384615384615385,\n",
        "        ('are', 'back'): 0.14285714285714285,\n",
        "        ('back', '</s>'): 0.15384615384615385}\n",
        "\n",
        "\n",
        "assert test_lm.vocab == expected_vocab_smoothing, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
        "\n",
        "assert test_lm.model == expected_model_smoothing, (\n",
        "    f\"Model mismatch! \\n\"\n",
        "    f\"Expected keys but missing: {set(expected_model_smoothing.keys()) - set(test_lm.model.keys())}\\n\"\n",
        "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model_smoothing.keys())}\\n\"\n",
        "    f\"Discrepancies in probabilities: \"\n",
        "    f\"{ {k: (expected_model_smoothing[k], test_lm.model[k]) for k in expected_model_smoothing if k in test_lm.model and expected_model_smoothing[k] != test_lm.model[k]} }\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157b0749",
      "metadata": {
        "id": "157b0749"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST unigram: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=1)\n",
        "test_lm = NGramLanguageModel(n=1, train_data=sample, alpha=1)\n",
        "\n",
        "expected_vocab_unigram = Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "expected_model_unigram = {('<s>',): 0.09090909090909091,\n",
        "        ('we',): 0.12121212121212122,\n",
        "        ('are',): 0.12121212121212122,\n",
        "        ('never',): 0.06060606060606061,\n",
        "        ('ever',): 0.15151515151515152,\n",
        "        ('getting',): 0.06060606060606061,\n",
        "        ('back',): 0.09090909090909091,\n",
        "        ('together',): 0.09090909090909091,\n",
        "        ('</s>',): 0.09090909090909091,\n",
        "        ('the',): 0.06060606060606061,\n",
        "        ('ones',): 0.06060606060606061}\n",
        "\n",
        "\n",
        "assert test_lm.vocab == expected_vocab_unigram, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
        "\n",
        "assert test_lm.model == expected_model_unigram, (\n",
        "    f\"Model mismatch! \\n\"\n",
        "    f\"Expected keys but missing: {set(expected_model_unigram.keys()) - set(test_lm.model.keys())}\\n\"\n",
        "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model_unigram.keys())}\\n\"\n",
        "    f\"Discrepancies in probabilities: \"\n",
        "    f\"{ {k: (expected_model_unigram[k], test_lm.model[k]) for k in expected_model_unigram if k in test_lm.model and expected_model_unigram[k] != test_lm.model[k]} }\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2927e9aa",
      "metadata": {
        "id": "2927e9aa"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: perplexity()\n",
        "#######################################\n",
        "test_lm = NGramLanguageModel(n=3, train_data=sample, alpha=0)\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "print(test_ppl)\n",
        "assert test_ppl < 1.7\n",
        "assert test_ppl > 0\n",
        "\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "print(test_ppl)\n",
        "assert test_ppl < 5.0\n",
        "assert test_ppl > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VW7eglKVDiAe",
      "metadata": {
        "id": "VW7eglKVDiAe"
      },
      "source": [
        "## Train the n-gram language model on the data/bbc/business.txt dataset for n = 2 and n = 3. Then do the same for data/bbc/sports.txt datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIslb4CFDbHK",
      "metadata": {
        "id": "IIslb4CFDbHK"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for business data\n",
        "#######################################\n",
        "business_prepro = preprocess(read_file(\"data/bbc/business.txt\"), n=2)\n",
        "train_bussi = NGramLanguageModel(n=2, train_data=business_prepro, alpha=0.5)\n",
        "print(len(set(train_bussi.model.keys())))\n",
        "print(len(train_bussi.n_grams_counts))\n",
        "print('Vocab size: ', len(train_bussi.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5WobtxwEDbC8",
      "metadata": {
        "id": "5WobtxwEDbC8"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for business data\n",
        "#######################################\n",
        "business_prepro = preprocess(read_file(\"data/bbc/business.txt\"), n=3)\n",
        "train_bussi = NGramLanguageModel(n=3, train_data=business_prepro, alpha=0.5)\n",
        "print(len(set(train_bussi.model.keys())))\n",
        "print(len(train_bussi.n_grams_counts))\n",
        "print('Vocab size: ', len(train_bussi.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UYlDBvvSDa97",
      "metadata": {
        "id": "UYlDBvvSDa97"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for sports data\n",
        "#######################################\n",
        "spo_prepro = preprocess(read_file(\"data/bbc/sport.txt\"), n=2)\n",
        "train_spo = NGramLanguageModel(n=2, train_data=spo_prepro, alpha=0.5)\n",
        "print(len(set(train_spo.model.keys())))\n",
        "print(len(train_spo.n_grams_counts))\n",
        "print('Vocab size: ', len(train_spo.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Qyq4NM8Da0v",
      "metadata": {
        "id": "5Qyq4NM8Da0v"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TRAIN unigram: NGramLanguageModel() for sports data\n",
        "#######################################\n",
        "spo_prepro = preprocess(read_file(\"data/bbc/sport.txt\"), n=3)\n",
        "train_spo = NGramLanguageModel(n=3, train_data=spo_prepro, alpha=0.5)\n",
        "print(len(set(train_spo.model.keys())))\n",
        "print(len(train_spo.n_grams_counts))\n",
        "print('Vocab size: ', len(train_spo.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zKE4iK0DGfZ_",
      "metadata": {
        "id": "zKE4iK0DGfZ_"
      },
      "source": [
        "How many possible 2- and 3- grams could there be, given the same vocabulary?\n",
        "\n",
        "\n",
        "How do the empirical counts given above compare to the number of possible 2- and 3- grams?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I90mZ5RkJsxs",
      "metadata": {
        "id": "I90mZ5RkJsxs"
      },
      "source": [
        "## Train a tri-gram (n=3, smoothing= 0.1) language models on collections of song lyrics from three popular artists (‘data/lyrics/‘) and use the model to score a new unattributed song."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a_aIvsckJySF",
      "metadata": {
        "id": "a_aIvsckJySF"
      },
      "outputs": [],
      "source": [
        "taylor_pre = preprocess(read_file(\"data/lyrics/taylor_swift.txt\"), n=3)\n",
        "train_tay = NGramLanguageModel(n=3, train_data=taylor_pre, alpha=0.1)\n",
        "\n",
        "green_pre = preprocess(read_file(\"data/lyrics/green_day.txt\"), n=3)\n",
        "train_green = NGramLanguageModel(n=3, train_data=green_pre, alpha=0.1)\n",
        "\n",
        "ed_pre = preprocess(read_file(\"data/lyrics/ed_sheeran.txt\"), n=3)\n",
        "train_ed = NGramLanguageModel(n=3, train_data=ed_pre, alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fmju45LGKbi1",
      "metadata": {
        "id": "fmju45LGKbi1"
      },
      "source": [
        "What are the perplexity scores of the test lyrics against each of the language models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Aw9AMywdJyK8",
      "metadata": {
        "id": "Aw9AMywdJyK8"
      },
      "outputs": [],
      "source": [
        "test_prepro = preprocess(read_file(\"data/lyrics/test_lyrics.txt\"), n=3)\n",
        "\n",
        "tay_ppl = train_tay.perplexity(test_prepro)\n",
        "print('Perplexity of taylor swift: ', tay_ppl)\n",
        "\n",
        "green_ppl = train_green.perplexity(test_prepro)\n",
        "print('Perplexity of green day: ', green_ppl)\n",
        "\n",
        "ed_ppl = train_ed.perplexity(test_prepro)\n",
        "print('Perplexity of ed sheeran: ', ed_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUdAiTpgVRpv",
      "metadata": {
        "id": "ZUdAiTpgVRpv"
      },
      "source": [
        "## Train a bi-gram (n=2, smoothing= 0.1) language models on collections of song lyrics from three popular artists (‘data/lyrics/‘) and use the model to score a new unattributed song."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1iEOWaj_VMyA",
      "metadata": {
        "id": "1iEOWaj_VMyA"
      },
      "outputs": [],
      "source": [
        "taylor_pre = preprocess(read_file(\"data/lyrics/taylor_swift.txt\"), n=2)\n",
        "train_tay = NGramLanguageModel(n=2, train_data=taylor_pre, alpha=0.1)\n",
        "\n",
        "green_pre = preprocess(read_file(\"data/lyrics/green_day.txt\"), n=2)\n",
        "train_green = NGramLanguageModel(n=2, train_data=green_pre, alpha=0.1)\n",
        "\n",
        "ed_pre = preprocess(read_file(\"data/lyrics/ed_sheeran.txt\"), n=2)\n",
        "train_ed = NGramLanguageModel(n=2, train_data=ed_pre, alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PLoaDiReVMtN",
      "metadata": {
        "id": "PLoaDiReVMtN"
      },
      "outputs": [],
      "source": [
        "test_prepro = preprocess(read_file(\"data/lyrics/test_lyrics.txt\"), n=2)\n",
        "\n",
        "tay_ppl = train_tay.perplexity(test_prepro)\n",
        "print('Perplexity of taylor swift: ', tay_ppl)\n",
        "\n",
        "green_ppl = train_green.perplexity(test_prepro)\n",
        "print('Perplexity of green day: ', green_ppl)\n",
        "\n",
        "ed_ppl = train_ed.perplexity(test_prepro)\n",
        "print('Perplexity of ed sheeran: ', ed_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fda0b2",
      "metadata": {
        "id": "06fda0b2"
      },
      "source": [
        "### Step 2: RNN Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454617c7",
      "metadata": {
        "id": "454617c7"
      },
      "source": [
        "#### Preparing the Data\n",
        "The following Python code is used for loading and processing [GloVe (Global Vectors for Word Representation) embeddings](https://nlp.stanford.edu/projects/glove/). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. These embeddings can be used in various natural language processing and machine learning tasks.\n",
        "\n",
        "The `load_glove_embeddings(path)` function is used to load the GloVe embeddings from a file. The function takes a file path as an argument, reads the file line by line, and for each line, it splits the line into words and their corresponding embeddings, and stores them in a dictionary. The dictionary, embeddings_dict, maps words to their corresponding vector representations.\n",
        "\n",
        "The `create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim)` function is used to create an embedding matrix from the loaded GloVe embeddings. This function takes a dictionary mapping words to their indices (`word_to_ix`), the dictionary of GloVe embeddings (`embeddings_dict`), and the dimension of the embeddings (`embedding_dim`) as arguments. It creates a zero matrix of size (vocab_size, embedding_dim) and then for each word in  `word_to_ix`, it checks if the word is in `embeddings_dict`. If it is, it assigns the corresponding GloVe vector to the word's index in the embedding matrix. If the word is not in the embeddings_dict, it assigns a random vector to the word's index in the embedding matrix.\n",
        "\n",
        "The `glove_path` variable is the path to the GloVe file, and `glove_embeddings` is the dictionary of GloVe embeddings loaded using the `load_glove_embeddings` function. The `embedding_dim` variable is the dimension of the embeddings, and `embedding_matrix` is the embedding matrix created using the create_embedding_matrix function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a4411999",
      "metadata": {
        "id": "a4411999"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/taylor_swift.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aQ3a37oj1-F6",
      "metadata": {
        "id": "aQ3a37oj1-F6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9e9b6d54",
      "metadata": {
        "id": "9e9b6d54"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Path to the GloVe file\n",
        "glove_path = 'glove.6B.50d.txt'  # Update this path\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "def create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim):\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in embeddings_dict:\n",
        "            embedding_matrix[ix] = embeddings_dict[word]\n",
        "        else:\n",
        "            embedding_matrix[ix] = torch.rand(embedding_dim)  # Random initialization for words not in GloVe\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cbade19",
      "metadata": {
        "id": "0cbade19"
      },
      "source": [
        "#### TO DO: Defining the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cKNdK6T7_OwQ",
      "metadata": {
        "id": "cKNdK6T7_OwQ"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: RNNLanguageModel()\n",
        "#######################################\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        \"\"\"\n",
        "        RNN language model con GRU y embeddings GloVe.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\n",
        "            \"mps\" if torch.backends.mps.is_available()\n",
        "            else \"cuda\" if torch.cuda.is_available()\n",
        "            else \"cpu\"\n",
        "        )\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Embedding inicializado con GloVe\n",
        "        # embedding_matrix: torch.Tensor [vocab_size, embedding_dim]\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight.copy_(embedding_matrix)\n",
        "\n",
        "        # GRU unidireccional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
        "                          hidden_size=hidden_dim,\n",
        "                          num_layers=1,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # Capa final a vocab\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        x: [B, T] índices\n",
        "        hidden: [1, B, H] opcional\n",
        "        retorna: logits [B, T, V], hidden\n",
        "        \"\"\"\n",
        "        x = x.to(self.device)\n",
        "        if hidden is not None:\n",
        "            hidden = hidden.to(self.device)\n",
        "\n",
        "        emb = self.embedding(x)            # [B, T, D]\n",
        "        out, hidden = self.rnn(emb, hidden)  # out: [B, T, H]\n",
        "        logits = self.fc(out)              # [B, T, V]\n",
        "        return logits, hidden\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
        "        \"\"\"\n",
        "        Autoregresivo desde la secuencia dada.\n",
        "        Usa último token como condición y mantiene el hidden.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # tokens iniciales\n",
        "        tokens = sequence.strip().split()\n",
        "        # map a ids con UNK si no está\n",
        "        unk = UNK if 'UNK' in globals() else '<unk>'\n",
        "        start_ids = [word_to_ix.get(w, word_to_ix.get(unk, 0)) for w in tokens]\n",
        "        if len(start_ids) == 0:\n",
        "            # si vacío, inicia con <s> si existe\n",
        "            start_ids = [word_to_ix.get(START, 0)]\n",
        "\n",
        "        # construir estado inicial ejecutando la secuencia\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=self.device).unsqueeze(0)  # [1, T]\n",
        "        logits, hidden = self.forward(x)  # oculto después de la secuencia\n",
        "\n",
        "        generated = []\n",
        "        last_id = x[0, -1].unsqueeze(0).unsqueeze(0)  # [1,1]\n",
        "\n",
        "        for _ in range(num_words):\n",
        "            logits, hidden = self.forward(last_id, hidden)  # [1,1,V]\n",
        "            probs = torch.softmax(logits[0, -1], dim=-1)\n",
        "\n",
        "            if mode == 'multinomial':\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            wid = next_id.item()\n",
        "            word = ix_to_word.get(wid, unk)\n",
        "            generated.append(word)\n",
        "\n",
        "            # parar si EOS\n",
        "            if word == EOS:\n",
        "                break\n",
        "\n",
        "            last_id = next_id.view(1, 1)\n",
        "\n",
        "        return generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6c516d",
      "metadata": {
        "id": "fe6c516d"
      },
      "source": [
        "#### Training the Model\n",
        "The following code snippet provided is responsible for training the RNN language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "96135209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96135209",
        "outputId": "e1c60252-28fd-4c3a-a3f9-64c44122c408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/20, Loss: 1.8877289295196533, Perplexity: 6.60435268587878\n",
            "Epoch 2/20, Loss: 2.3066248893737793, Perplexity: 10.040479673564507\n",
            "Epoch 3/20, Loss: 1.6212486028671265, Perplexity: 5.05940356016781\n",
            "Epoch 4/20, Loss: 3.0175232887268066, Perplexity: 20.440603467026385\n",
            "Epoch 5/20, Loss: 1.1223251819610596, Perplexity: 3.0719888384478238\n",
            "Epoch 6/20, Loss: 2.61008882522583, Perplexity: 13.60025884424322\n",
            "Epoch 7/20, Loss: 1.8380917310714722, Perplexity: 6.284534229808462\n",
            "Epoch 8/20, Loss: 1.6776500940322876, Perplexity: 5.352962221983752\n",
            "Epoch 9/20, Loss: 2.4872968196868896, Perplexity: 12.028716343585533\n",
            "Epoch 10/20, Loss: 2.544201612472534, Perplexity: 12.733058112032381\n",
            "Epoch 11/20, Loss: 1.3362374305725098, Perplexity: 3.8047010881214876\n",
            "Epoch 12/20, Loss: 2.1243982315063477, Perplexity: 8.367860457933368\n",
            "Epoch 13/20, Loss: 2.313692331314087, Perplexity: 10.111691527118902\n",
            "Epoch 14/20, Loss: 2.348663330078125, Perplexity: 10.47156334210341\n",
            "Epoch 15/20, Loss: 2.6359918117523193, Perplexity: 13.957148464550283\n",
            "Epoch 16/20, Loss: 2.8863821029663086, Perplexity: 17.928329277056434\n",
            "Epoch 17/20, Loss: 2.5999062061309814, Perplexity: 13.462475278140287\n",
            "Epoch 18/20, Loss: 2.256592035293579, Perplexity: 9.550485924010976\n",
            "Epoch 19/20, Loss: 2.825664758682251, Perplexity: 16.872157171945545\n",
            "Epoch 20/20, Loss: 1.940276861190796, Perplexity: 6.960677845443573\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: RNNLanguageModel() and training\n",
        "#######################################\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 20\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(RNN.device)\n",
        "        targets = targets.to(RNN.device)\n",
        "\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
